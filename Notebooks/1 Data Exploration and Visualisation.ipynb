{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "009529390e01cdb6ffcca81d77e6a77cbc92c0c2"
   },
   "source": [
    "# Classifying Urban sounds using Deep Learning\n",
    "\n",
    "## 1 Data Exploration and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UrbanSound dataset\n",
    "\n",
    "For this project we will use a dataset called Urbansound8K. The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, which are:\n",
    "\n",
    "- Air Conditioner\n",
    "- Car Horn\n",
    "- Children Playing\n",
    "- Dog bark\n",
    "- Drilling\n",
    "- Engine Idling\n",
    "- Gun Shot\n",
    "- Jackhammer\n",
    "- Siren\n",
    "- Street Music\n",
    "\n",
    "The accompanying metadata contains a unique ID for each sound excerpt along with it's given class name.\n",
    "\n",
    "A sample of this dataset is included with the accompanying git repo and the full dataset can be downloaded from [here](https://urbansounddataset.weebly.com/urbansound8k.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio sample file data overview\n",
    "\n",
    "These sound excerpts are digital audio files in .wav format. \n",
    "\n",
    "Sound waves are digitised by sampling them at discrete intervals known as the sampling rate (typically 44.1kHz for CD quality audio meaning samples are taken 44,100 times per second). \n",
    "\n",
    "Each sample is the amplitude of the wave at a particular time interval, where the bit depth determines how detailed the sample will be also known as the dynamic range of the signal (typically 16bit which means a sample can range from 65,536 amplitude values). \n",
    "\n",
    "Therefore, the data we will be analysing for each sound excerpts is essentially a one dimensional array or vector of amplitude values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing audio data \n",
    "\n",
    "For audio analysis, we will be using the following libraries: \n",
    "\n",
    "#### 1. IPython.display.Audio \n",
    "\n",
    "This allows us to play audio directly in the Jupyter Notebook. \n",
    "\n",
    "#### 2. Librosa \n",
    "\n",
    "librosa is a Python package for music and audio processing by Brian McFee and will allow us to load audio in our notebook as a numpy array for analysis and manipulation. \n",
    "\n",
    "You may need to install librosa using pip as follows: \n",
    "\n",
    "`pip install librosa'     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditory inspection \n",
    "\n",
    "We will use `IPython.display.Audio` to play the audio files so we can inspect aurally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio('../audio/1-977-A-39.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual inspection\n",
    "\n",
    "We will load a sample from each class and visually inspect the data for any patterns. We will use librosa to load the audio file into an array then librosa.display and matplotlib to display the waveform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "\n",
    "\n",
    "From a visual inspection we can see that it is tricky to visualise the difference between some of the classes. \n",
    "\n",
    "Particularly, the waveforms for reptitive sounds for air conditioner, drilling, engine idling and jackhammer are similar in shape.  \n",
    "\n",
    "Likewise the peak in the dog barking sample is simmilar in shape to the gun shot sample (albeit the samples differ in that there are two peaks for two gunshots compared to the one peak for one dog bark). Also, the car horn is similar too. \n",
    "\n",
    "There are also similarities between the children playing and street music. \n",
    "\n",
    "The human ear can naturally detect the difference between the harmonics, it will be interesting to see how well a deep learning model will be able to extract the necessary features to distinguish between these classes. \n",
    "\n",
    "\n",
    "However, it is easy to differentiate from the waveform shape, the difference between certain classes such as dog barking and jackhammer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class: Air Conditioner\n",
    "\n",
    "filename = '../audio/1-977-A-39.wav'\n",
    "plt.figure(figsize=(12,4))\n",
    "data,sample_rate = librosa.load(filename)\n",
    "_ = librosa.display.waveplot(data,sr=sample_rate)\n",
    "ipd.Audio(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imports\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Metadata \n",
    "\n",
    "Here we will load the UrbanSound metadata .csv file into a Panda dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80 entries, 0 to 79\n",
      "Data columns (total 7 columns):\n",
      "filename    80 non-null object\n",
      "fold        80 non-null int64\n",
      "target      80 non-null int64\n",
      "category    80 non-null object\n",
      "esc10       80 non-null bool\n",
      "src_file    80 non-null int64\n",
      "take        80 non-null object\n",
      "dtypes: bool(1), int64(3), object(3)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv('/home/animesh/ESC-50-master/meta/esc50.csv')\n",
    "metadata.head(10)\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata.category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[metadata['target'] != 24,'target'] = 2\n",
    "metadata.loc[metadata['target'] == 24,'target'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for index in metadata.target:\n",
    "    if index == 1:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "\n",
    "Here we can see the Class labels are unbalanced. Although 7 out of the 10 classes all have exactly 1000 samples, and siren is not far off with 929, the remaining two (car_horn, gun_shot) have significantly less samples at 43% and 37% respectively. \n",
    "\n",
    "This will be a concern and something we may need to address later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio sample file properties\n",
    "\n",
    "Next we will iterate through each of the audio sample files and extract, number of audio channels, sample rate and bit-depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from helpers.wavfilehelper import WavFileHelper\n",
    "wavfilehelper = WavFileHelper()\n",
    "\n",
    "audiodata = []\n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    #file_name = os.path.join(os.path.abspath('/home/animesh/ESC-50-master/audio/'),'fold'+str(row[\"fold\"])+'/',str(row[\"file_name\"]))\n",
    "    file_name = os.path.join(os.path.abspath('/home/animesh/ESC-50-master/audio/'),str(row[\"filename\"]))\n",
    "    print(file_name)\n",
    "    data = wavfilehelper.read_file_properties(file_name)\n",
    "    audiodata.append(data)\n",
    "\n",
    "# Convert into a Panda dataframe\n",
    "audiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio channels \n",
    "\n",
    "Most of the samples have two audio channels (meaning stereo) with a few with just the one channel (mono). \n",
    "\n",
    "The easiest option here to make them uniform will be to merge the two channels in the stero samples into one by averaging the values of the two channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1.0\n",
      "Name: num_channels, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# num of channels \n",
    "\n",
    "print(audiodf.num_channels.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample rate \n",
    "\n",
    "There is a wide range of Sample rates that have been used across all the samples which is a concern (ranging from 96k to 8k).\n",
    "\n",
    "This likley means that we will have to apply a sample-rate conversion technique (either up-conversion or down-conversion) so we can see an agnostic representation of their waveform which will allow us to do a fair comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100    1.0\n",
      "Name: sample_rate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# sample rates \n",
    "\n",
    "print(audiodf.sample_rate.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bit-depth \n",
    "\n",
    "There is also a wide range of bit-depths. It's likely that we may need to normalise them by taking the maximum and minimum amplitude values for a given bit-depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16    1.0\n",
      "Name: bit_depth, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# bit depth\n",
    "\n",
    "print(audiodf.bit_depth.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other audio properties to consider \n",
    "\n",
    "We may also need to consider normalising the volume levels (wave amplitude value) if this is seen to vary greatly, by either looking at the peak volume or the RMS volume. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *In the next notebook we will preprocess the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>fold</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>esc10</th>\n",
       "      <th>src_file</th>\n",
       "      <th>take</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-11687-A-47.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>airplane</td>\n",
       "      <td>False</td>\n",
       "      <td>11687</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-12653-A-15.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>water_drops</td>\n",
       "      <td>False</td>\n",
       "      <td>12653</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-12654-A-15.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>water_drops</td>\n",
       "      <td>False</td>\n",
       "      <td>12654</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-12654-B-15.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>water_drops</td>\n",
       "      <td>False</td>\n",
       "      <td>12654</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-13571-A-46.wav</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>church_bells</td>\n",
       "      <td>False</td>\n",
       "      <td>13571</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  fold  target      category  esc10  src_file take\n",
       "0  1-11687-A-47.wav     1       2      airplane  False     11687    A\n",
       "1  1-12653-A-15.wav     1       2   water_drops  False     12653    A\n",
       "2  1-12654-A-15.wav     1       2   water_drops  False     12654    A\n",
       "3  1-12654-B-15.wav     1       2   water_drops  False     12654    B\n",
       "4  1-13571-A-46.wav     1       2  church_bells  False     13571    A"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
